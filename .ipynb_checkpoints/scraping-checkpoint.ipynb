{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    url = \"https://news.google.com/news/headlines?ned=in&hl=en-IN&gl=IN\"\n",
    "    data = requests.get(url)\n",
    "    soup = BeautifulSoup(data.content, \"html.parser\")\n",
    "    \n",
    "    path = r\"C:\\Users\\Vishal-PC\\k-means\"\n",
    "    filename = os.path.join(path, 'data\\scraped_headlines.txt')\n",
    "    links = soup.find_all(\"a\")\n",
    "    with open(filename, 'w', encoding=\"utf-8\") as f:\n",
    "        for link in links:\n",
    "            text = link.text\n",
    "            headline_length = len(text.split())\n",
    "            if headline_length > 4:\n",
    "                f.write(text)\n",
    "                f.write('\\n')\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "import os\n",
    "def main():\n",
    "    path = r\"C:\\Users\\Vishal-PC\\k-means\"\n",
    "    f = open(os.path.join(path, 'data\\scraped_headlines.txt'), 'rt', encoding='utf-8') #read in text format\n",
    "    text_file = f.read().split('\\n')\n",
    "    text_lower = [text.lower() for text in text_file]\n",
    "    text_letters = [''.join(c for c in s if c not in punctuation)for s in text_lower]\n",
    "    text_final = [re.sub(r'[^A-Za-z]+',' ', x)for x in text_letters]\n",
    "    with open(os.path.join(path, 'data\\headlines_cleaned.txt'),'w') as fw:\n",
    "        for text in text_final:\n",
    "            fw.write(text)\n",
    "            fw.write('\\n')\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-43cecb466092>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#path=\"e://clustring//\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'file' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokenize and Stem Data\n",
    "# Convert words to Vector Space using TFIDF matrix\n",
    "# Using KMeans clustering to find out clusters\n",
    "# Calculate Cosine Similarity and generate the distance matrix\n",
    "# Dimensionality reduction using MDS to results the KMeans output\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import MDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "\n",
    "path = os.path.abspath(os.path.dirname(file))\n",
    "\n",
    "#path=\"e://clustring//\"\n",
    "# Tokenizer to return stemmed words, we use this\n",
    "def tokenize_and_stem(text_file):\n",
    "    # declaring stemmer and stopwords language\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text_file)\n",
    "    filtered = [w for w in words if w not in stop_words]\n",
    "    stems = [stemmer.stem(t) for t in filtered]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    data = pd.read_csv(os.path.join(path, 'data\\headlines_cleaned.txt'),names=['text'])\n",
    "\n",
    "    # text data in dataframe and removing stops words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "    # Using TFIDF vectorizer to convert convert words \n",
    "    #to Vector Space\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=200000,use_idf=True,\n",
    "    stop_words='english',tokenizer=tokenize_and_stem)\n",
    "\n",
    "    # Fit the vectorizer to text data\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(data['text'])\n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "    # print(terms)\n",
    "\n",
    "    # Kmeans++\n",
    "    km = KMeans(n_clusters=7, init='k-means++', max_iter=300, n_init=1, verbose=0,random_state=34)\n",
    "    km.fit(tfidf_matrix)\n",
    "    labels = km.labels_\n",
    "    clusters = labels.tolist()\n",
    " #print(cluster)\n",
    "    # Calculating the distance measure derived from cosine \n",
    "    #similarity\n",
    "    distance = 1 - cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Dimensionality reduction using Multidimensional \n",
    "    #scaling (MDS)\n",
    "    mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "    pos = mds.fit_transform(distance)\n",
    "    xs, ys = pos[:, 0], pos[:, 1]\n",
    "\n",
    "    # Saving cluster visualization after mutidimensional scaling\n",
    "    for x, y, in zip(xs, ys):\n",
    "        plt.scatter(x, y)\n",
    "    plt.title('MDS output of News Headlines')\n",
    "    plt.savefig(os.path.join(path, 'results\\MDS.png'))\n",
    "\n",
    "    # Creating dataframe containing reduced dimensions, \n",
    "    #identified labels and text data for plotting \n",
    "    #KMeans output\n",
    "    df = pd.DataFrame(dict(label=clusters, \n",
    "                   data=data['text'], x=xs, y=ys))\n",
    "    df.to_csv(os.path.join(path, \n",
    "           'results\\kmeans_clustered_DF.txt'), sep=',')\n",
    "\n",
    "    label_color_map = {0: 'red',\n",
    "                       1: 'blue',\n",
    "                       2: 'green',\n",
    "                       3: 'pink',\n",
    "                       4: 'purple',\n",
    "                       5: 'yellow',\n",
    "                       6: 'orange',\n",
    "                       7: 'grey'\n",
    "                       }\n",
    "\n",
    "    csv = open(os.path.join(path, 'results\\kmeans_clustered_output.txt'), 'w')\n",
    "    csv.write('Cluster     Headline\\n')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(17, 9))\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        cluster = row['label']\n",
    "        label_color = label_color_map[row['label']]\n",
    "        label_text = row['data']\n",
    "        ax.plot(row['x'], row['y'], marker='o', \n",
    "        ms=12, c=label_color)\n",
    "        row = str(cluster) + ',' + label_text + '\\n'\n",
    "        csv.write(row)\n",
    "\n",
    "    # ax.legend(numpoints=1)\n",
    "    for i in range(len(df)):\n",
    "        ax.text(df.ix[i]['x'], df.ix[i]['y'],\n",
    "        df.ix[i]['label'], size=8)\n",
    "\n",
    "    plt.title('News Headlines using KMeans Clustering')\n",
    "    plt.savefig(os.path.join(path, 'results\\kmeans.png'))\n",
    "\n",
    "\n",
    "if name == 'main':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
